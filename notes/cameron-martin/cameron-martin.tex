\documentclass{article}
\usepackage{amsmath,amssymb,mathrsfs,amsthm}
%\usepackage{tikz-cd}
\usepackage{hyperref}
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\tr}{\ensuremath\mathrm{tr}\,} 
\newcommand{\Span}{\ensuremath\mathrm{span}} 
\def\Re{\ensuremath{\mathrm{Re}}\,}
\def\Im{\ensuremath{\mathrm{Im}}\,}
\newcommand{\id}{\ensuremath\mathrm{id}} 
\newcommand{\diam}{\ensuremath\mathrm{diam}} 
\newcommand{\Cov}{\ensuremath\mathrm{Cov}} 
\newcommand{\Var}{\ensuremath\mathrm{Var}} 
\newcommand{\lcm}{\ensuremath\mathrm{lcm}} 
\newcommand{\supp}{\ensuremath\mathrm{supp}\,}
\newcommand{\grad}{\ensuremath\mathrm{grad}\,}
\newcommand{\dom}{\ensuremath\mathrm{dom}\,}
\newcommand{\upto}{\nearrow}
\newcommand{\downto}{\searrow}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\begin{document}
\title{The Cameron-Martin theorem}
\author{Jordan Bell\\ \texttt{jordan.bell@gmail.com}\\Department of Mathematics, University of Toronto}
\date{\today}

\maketitle

\section{Gaussian vectors in a Hilbert space}
\label{hilbert}
\begin{lemma}
Let $(\Omega,\mathfrak{F})$ be a measurable space and let $(Y,d)$ be a metric space. 
Suppose that $(f_n)$ is a sequence of measurable functions
$(\Omega,\mathfrak{F}) \to (Y,\mathfrak{B}_Y)$, 
$A \in \mathfrak{F}$, $y_0 \in Y$, and  
$f_n(\omega)$ converges in $Y$ for all $\omega \in A$. Then 
$f:\Omega \to Y$ defined by
\[
f(\omega) = \begin{cases}
\lim_{n \to \infty} f_n(\omega)&\omega \in A\\
y_0&\omega \not \in A
\end{cases}
\]
is measurable.
\label{pointwise}
\end{lemma}
\begin{proof}
Because the Borel $\sigma$-algebra $\mathfrak{B}_Y$ is generated by 
the collection of closed sets in $Y$, it suffices to prove that $f^{-1}(F) \in \mathfrak{F}$ when $F$ is a closed set
in $Y$. Let
\[
G_n = \left\{y \in Y: d(y,F)< \frac{1}{n} \right\}.
\]
Because $y \mapsto d(y,F)$ is continuous, each $G_n$ is open. Because $F$ is closed,
$F = \bigcap_{n=1}^\infty G_n$.


If $\omega \in A \cap f^{-1}(F)$ and $k \geq 1$,  then
because $G_k$ is an open neighborhood of $f(\omega)$ and $f_n(\omega) \to f(\omega) \in G_k$,
there is some $m_k$ such that for $n \geq m_k$ the point $f_n(\omega)$ belongs to $G_k$.
Thus 
\[
A \cap f^{-1}(F) \subset A \cap \bigcap_{k=1}^\infty \bigcup_{m=1}^\infty \bigcap_{n=m}^\infty f_n^{-1}(G_k).
\]
On the other hand, if $\omega$ belongs to the right-hand side then
for each $k$ there is some $m_k$ such that for $n \geq m_k$, $f_n(\omega) \in G_k$. Because
$f_n(\omega) \to f(\omega)$, this means that $f(\omega) \in \overline{G_k}$. 
This is true for all $k$, so $f(\omega) \in \bigcap_{k=1}^\infty \overline{G_k}$, and because 
$\overline{G_{k+1}} \subset G_k$, it is the case that
$f(\omega) \subset \bigcap_{k=1}^\infty G_k = F$. 
Therefore,
\[
A \cap f^{-1}(F) = A \cap \bigcap_{k=1}^\infty \bigcup_{m=1}^\infty \bigcap_{n=m}^\infty f_n^{-1}(G_k),
\]
which shows that $A \cap f^{-1}(F) \in \mathfrak{F}$. 
If $y_0 \in F$, then $f^{-1}(F) = A^c \cup  (A \cap f^{-1}(F)) \in \mathfrak{F}$, and if
$y_0 \not \in F$, then $f^{-1}(F) = A \cap f^{-1}(F) \in \mathfrak{F}$. Therefore $f$ is measurable.
\end{proof}


Let $\mathscr{H}$ be a separable real Hilbert space with inner product $\inner{\cdot}{\cdot}$
and
let $(e_j)$ be an orthonormal basis for $\mathscr{H}$. Let
$(\xi_j)$ be a sequence of independent random variables
$(\Omega,\mathfrak{F},\mathbb{P}) \to (\mathbb{R},\mathfrak{B}_{\mathbb{R}})$ with distribution
$(\xi_j)_*\mathbb{P}=\gamma_1$, where $\gamma_{\sigma^2}$ is the  Gaussian measure on $\mathbb{R}$ with variance $\sigma^2$.\footnote{\url{http://individual.utoronto.ca/jordanbell/notes/gaussian.pdf}}
Let $(\sigma_j)$ be a sequence of nonnegative real numbers satisfying $\sum_{j=1}^\infty \sigma_j^2<\infty$.
Define $X_n:\Omega \to \mathscr{H}$ by
\[
X_n(\omega) = \sum_{j=1}^n \sigma_j \xi_j(\omega) e_j,
\]
which is measurable $(\Omega,\mathfrak{F}) \to (\mathscr{H},\mathfrak{B}_{\mathscr{H}})$. 
For $X_n(\omega)$ to be a Cauchy sequence in $\mathscr{H}$, 
it is necessary and sufficient that
$\sum_{j=1}^\infty |\sigma_j \xi_j(\omega)|^2<\infty$.\footnote{\url{http://individual.utoronto.ca/jordanbell/notes/parseval.pdf}}
But
\[
\sum_{j=1}^\infty \mathbb{E}|\sigma_j \xi_j|^2 = \sum_{j=1}^\infty \sigma_j^2 \mathbb{E}|\xi_j|^2 = 
\sum_{j=1}^\infty \sigma_j^2<\infty
\]
 implies that the series $\sum_{j=1}^\infty |\sigma_j \xi_j|^2$ is convergent almost surely:
  for some $A \in \mathfrak{F}$ with $\mathbb{P}(A)=1$ the series $\sum_{j=1}^\infty |\sigma_j \xi_j(\omega)|^2$ converges for $\omega \in A$.
For  $\omega \in A$ we define $X(\omega) \in 
\mathscr{H}$ to be the limit of the Cauchy sequence $X_n(\omega)$,
\begin{equation}
X(\omega) = \sum_{j=1}^\infty \sigma_j \xi_j(\omega) e_j,
\label{karhunen}
\end{equation}
and otherwise we define $X(\omega)=0$.
By Lemma \ref{pointwise}, $X$ is measurable $(\Omega,\mathfrak{F}) \to (\mathscr{H},\mathfrak{B}_{\mathscr{H}})$.\footnote{Mikhail Lifshits, {\em Lectures on Gaussian Processes},
p.~7, Example 2.2; Lifshits calls this a \textbf{Karhunen-Lo\`eve expansion} of $X$.}


For $X$ defined in \eqref{karhunen} and for $f \in \mathscr{H}$ with
\[
f = \sum_j \inner{f}{e_j} e_j = \sum_j f_j e_j,
\]
we have for $\omega \in A$,
\[
\inner{f}{X} = \sum_{j=1}^\infty f_j \sigma_j \xi_j(\omega).
\]
This satisfies
\[
\mathbb{E}\inner{X}{f} = 0,
\]
and
for $f,g \in \mathscr{H}$,
\begin{align*}
\Cov(\inner{X}{f},\inner{X}{g})&=E(\inner{X}{f} \cdot \inner{X}{g})\\
&=E\left( \sum_{j=1}^\infty \sigma_j f_j \xi_j \cdot \sum_{k=1}^\infty \sigma_k g_k \xi_k\right)\\
&=\sum_{j=1}^\infty \sigma_j^2 f_j g_j.
\end{align*}
Define $K:\mathscr{H} \to \mathscr{H}$ by
\[
Ke_j = \sigma_j^2 e_j,
\]
which is a Hilbert-Schmidt operator.\footnote{\url{http://individual.utoronto.ca/jordanbell/notes/traceclass.pdf}}
It satisfies
\[
\inner{Kf}{g} = \Cov(\inner{X}{f},\inner{X}{g}).
\]



\section{Wiener measure}
\label{wiener}
Let $\mathscr{X}=C[0,1]$, which is a separable Banach space with the supremum norm, whose
dual space $\mathscr{X}^*$ is the signed measures of bounded variation on $[0,1]$.\footnote{\url{http://individual.utoronto.ca/jordanbell/notes/CK.pdf}}
For $\mu \in \mathscr{X}^*$ and $f \in \mathscr{X}$, write
\[
\inner{f}{\mu} = \int_{[0,1]} f d\mu.
\]
Let $W \in \mathscr{X}^*$ be Wiener measure on $\mathscr{X}$,  define 
$B_tf = f(t)$, and define
$B:\mathscr{X} \to \mathscr{X}$ by
$Bf = f$.\footnote{\url{http://individual.utoronto.ca/jordanbell/notes/donsker.pdf}}
The stochastic process $(B_t)_{t \in [0,1]}$ is  a Brownian motion. For $s,t \in [0,1]$,
\[
\mathbb{E} B_t = 0,\qquad \Cov(B_s,B_t)=\mathbb{E}(B_s B_t) = \min(s,t).
\]
$B:(\mathscr{X},\mathfrak{B}_{\mathscr{X}}) \to (\mathscr{X},\mathfrak{B}_{\mathscr{X}})$ is measurable, and
$B_* W = W$, i.e. the distribution of $B$ is Wiener measure.
For $\mu \in \mathscr{X}^*$,
\[
\mathbb{E} \inner{B}{\mu} = \mathbb{E} \int_{[0,1]} B_t d\mu(t) = \int_{[0,1]} \mathbb{E}B_t d\mu = 0
\]
and for $\mu,\nu \in \mathscr{X}^*$,
\begin{align*}
\Cov(\inner{B}{\mu},\inner{B}{\nu})&=\mathbb{E}\left(\int_{[0,1]} B_s d\mu(s) \cdot \int_{[0,1]} B_t d\nu(t) \right)\\
&=\int_{[0,1] \times [0,1]} \mathbb{E}(B_s B_t) d\mu(s) d\nu(t)\\
&=\int_{[0,1] \times [0,1]} \min(s,t) d\mu(s) d\nu(t).
\end{align*}
Define $K:\mathscr{X}^* \to \mathscr{X}$ by
\[
(K\mu)(t) = \int_{[0,1]} \min(s,t) d\mu(s),
\]
which satisfies
\[
\Cov(\inner{B}{\mu},\inner{B}{\nu}) = \inner{K\mu}{\nu}.
\]




\section{Measurable linear functionals}
Let $\mathscr{X}$ be a Fr\'echet space with dual space $\mathscr{X}^*$, and for $f \in \mathscr{X}$ and $\mu
\in \mathscr{X}^*$ denote the dual pairing by
\[
\inner{f}{\mu},
\]
and we also use this notation when $\mu$ is a function  $\mathscr{X} \to \mathbb{R}$ that need not belong to $\mathscr{X}^*$.
Suppose that $X:(\Omega,\mathfrak{F},\mathbb{P}) \to (\mathscr{X},\mathfrak{B}_{\mathscr{X}})$ is measurable,
and that
$X$ is Gaussian with $\mathbb{E} X=0 \in \mathscr{X}$ and covariance $K:\mathscr{X}^* \to \mathscr{X}$. That is, 
\[
\mathbb{E} \inner{X}{\mu} = \inner{0}{\mu}=0
\]
for all $\mu \in \mathscr{X}^*$, and $K:\mathscr{X}^* \to \mathscr{X}$ is a continuous linear operator satisfying
\[
\mathbb{E}(\inner{X}{\mu} \cdot \inner{X}{\nu}) = \Cov(\inner{X}{\mu},\inner{X}{\nu}) = \inner{K\mu}{\nu}
\]
for all $\mu,\nu \in \mathscr{X}^*$. 
Let $P=X_* \mathbb{P}$ be the distribution of $X$; $P$ is a Borel probability measure on $\mathscr{X}$.


For $\mu \in \mathscr{X}^*$, by the change of variables formula,
\[
\mathbb{E} |\inner{X}{\mu}|^2 = 
\int_\Omega |\inner{X(\omega)}{\mu}|^2 d\mathbb{P}(\omega)
=\int_{\mathscr{X}} |\inner{f}{\mu}|^2 dP(f)
=\int_{\mathscr{X}} |\mu|^2 dP.
\]
Let $J:\mathscr{X}^* \to L^2(\mathscr{X},P)$ be the embedding, and let
$\mathscr{X}_P^*$ be the closure of $J(\mathscr{X}^*)$ in $L^2(\mathscr{X},P)$. Thus 
$\mathscr{X}_P^*$ is a Hilbert space with the inner product
\[
\inner{\phi}{\psi}_{\mathscr{X}_P^*} = \int_{\mathscr{X}} \phi \cdot \psi dP = \mathbb{E}(\inner{X}{\phi} \cdot \inner{X}{\psi}).
\]
Elements of $\mathscr{X}_P^*$ are called \textbf{measurable linear functionals}; elements of $\mathscr{X}^*$
are continuous linear functionals. 


$J:\mathscr{X}^* \to \mathscr{X}_P^*$ is a continuous linear map, and there is a unique continuous linear
map $I:(\mathscr{X}_P^*)^* \to \mathscr{X}$ satisfying
\[
\inner{I \phi}{\mu} = \inner{\phi}{J\mu}_{\mathscr{X}_P^*} = \inner{\phi}{\mu}_{\mathscr{X}_P^*} = \mathbb{E}(\inner{X}{\phi}\cdot \inner{X}{\mu})
\]
for $\phi \in (\mathscr{X}_P^*)^*=\mathscr{X}_P^*$
and $\mu \in \mathscr{X}^*$.
If $I \phi = 0$ then 
\[
\inner{\phi}{J \mu}_{\mathscr{X}_P^*} = \inner{I \phi}{\mu} = \inner{0}{\mu} = 0
\]
for all $\mu \in \mathscr{X}^*$. Let $\mu_n \in \mathscr{X}^*$ with
$J\mu_n \to \phi$ in $L^2(\mathscr{X},P)$. Then $\inner{\phi}{J\mu_n}_{\mathscr{X}_P^*} \to \inner{\phi}{\phi}_{\mathscr{X}_P^*}$, and because
each $\inner{\phi}{J\mu_n}_{\mathscr{X}_P^*}=0$ we get that $\inner{\phi}{\phi}_{\mathscr{X}_P^*}=0$, which means that
$\phi=0$. Therefore $I$ is injective. 

We have assumed that $X$ has covariance $K:\mathscr{X}^* \to \mathscr{X}$, which means that
for $\mu,\nu \in \mathscr{X}^*$,
\[
\inner{K\mu}{\nu} = \mathbb{E}(\inner{X}{\mu}\cdot \inner{X}{\nu}).
\]
For $\mu,\nu \in \mathscr{X}^*$,
\[
\inner{IJ\mu}{\nu}=\inner{J\mu}{J\nu}_{\mathscr{X}_P^*}
=\mathbb{E}(\inner{X}{\mu}\cdot \inner{X}{\nu})
=\inner{K\mu}{\nu},
\]
which implies that $K=IJ$. 

Let
\[
H_P = I \mathscr{X}_P^*,
\]
which is a linear subspace of $\mathscr{X}$. For $f,g \in H_P$,
let
\[
\inner{f}{g}_{H_P} = \inner{I^{-1}f}{I^{-1}g}_{\mathscr{X}_P^*}.
\]


\section{Examples of $H_P$}
Take $X:\Omega \to \mathscr{H}$ from \S \ref{hilbert}, with $\mathbb{E} X = 0$ and 
with covariance
 $K:\mathscr{H} \to \mathscr{H}$ defined by
\[
Ke_j = \sigma_j^2 e_j,
\]
which is a Hilbert-Schmidt operator satisfying
\[
\inner{Kf}{g} = \Cov(\inner{X}{f},\inner{X}{g}).
\]
For $f,g \in \mathscr{H} = \mathscr{H}^*$,
\[
\inner{Jf}{Jg}_{\mathscr{X}_P^*}
=\inner{Kf}{g}
=\sum_{j=1}^\infty \sigma_j^2 f_j g_j.
\]
Check that
$\mathscr{X}_P^*$ is the set of those $\phi:\mathscr{H} \to \mathbb{R}$ such that 
\[
\sum_{j=1}^\infty |\inner{e_j}{\phi}|^2 \sigma_j^2 < \infty.
\]
Writing $\phi_j = \inner{\phi}{e_j}$,
\begin{align*}
\inner{I\phi}{e_k} &= \inner{\phi}{Je_k}_{\mathscr{X}_P^*}\\
&=\inner{\phi}{e_k}_{\mathscr{X}_P^*}\\
&=\mathbb{E}(\inner{X}{\phi}\cdot \inner{X}{e_k})\\
&=\mathbb{E}\left( \sum_{j=1}^\infty \phi_j \sigma_j \xi_j \cdot
\sigma_k \xi_k\right)\\
&= \sigma_j^2 \phi_j.
\end{align*}
For $\phi \in \mathscr{X}_P^*$, write $h=I\phi$, and then
\[
\norm{h}_{H_P}^2 = \norm{\phi}_{\mathscr{X}_P^*}^2
=\sum_{j=1}^\infty \sigma_j^2 \phi_j^2.
\]
But
\begin{align*}
h_k&=\inner{h}{e_k}\\
&= \inner{I\phi}{e_k} = \inner{\phi}{Je_k}_{\mathscr{X}_P^*}\\
& = 
\mathbb{E}(\inner{X}{\phi}\cdot \inner{X}{e_k})\\
&=\mathbb{E}\left( \sum_{j=1}^\infty \phi_j \sigma_j \xi_j \cdot \sigma_k \xi_k\right)\\
&=\sigma_k^2 \phi_k,
\end{align*}
so
\[
\norm{h}_{H_P}^2 = \sum_{j=1}^\infty \frac{h_j^2}{\sigma_j^2}.
\]
We then check that
\[
H_P = \left\{ h \in \mathscr{H} : \sum_{j=1}^\infty \frac{h_j^2}{\sigma_j^2} < \infty\right\}.
\]


Now take $\mathscr{X}=\mathbb{R}^d$ and let $X:(\Omega,\mathfrak{F},\mathbb{P}) \to 
(\mathbb{R}^d,\mathscr{B}_{\mathbb{R}}^d)$ be a random vector that is Gaussian with $\mathbb{E} X=0$
and
positive-definite covariance $K:\mathbb{R}^d \to \mathbb{R}^d$,
and let $P=X_* \mathbb{P}$, a Borel probability measure on $\mathbb{R}^d$.
Because $K$ is a positive-definite symmetric matrix, 
by the spectral theorem there is an orthonormal basis $e_1,\ldots,e_d$ for $\mathbb{R}^d$
and positive real numbers $\sigma_1,\ldots,\sigma_d$ such that
$Ke_j = \sigma_j^2 e_j$. 
For almost all $\omega \in \Omega$,
\[
X(\omega) = \sum_{j=1}^d \sigma_j \xi_j(\omega) e_j.
\]
From our work before, $H_P = \mathbb{R}^d$, and
\[
\inner{f}{g}_{H_P} =\sum_{j=1}^\infty \frac{f_j g_j}{\sigma_j^2}
=\inner{K^{-1}f}{g}.
\]



\section{The factorization theorem}
The following is proved in Lifshits, and there is called the \textbf{factorization theorem}.\footnote{Mikhail Lifshits, {\em Lectures on Gaussian Processes},
p.~26, Theorem 4.1.}

\begin{theorem}[Factorization theorem]
If $\mathscr{X}$ is a Fr\'echet space, $\mathscr{H}$ is a Hilbert space, and
$L:\mathscr{H} \to \mathscr{X}$ is an injective linear map
such that
\[
K = LL^*,
\]
then
\[
H_P = L \mathscr{H}
\]
and
\[
\inner{f}{g}_{H_P} = \inner{L^{-1}f}{L^{-1}g}_{\mathscr{H}}
\]
for all $f,g \in H_P$.
\label{factorization}
\end{theorem}


Let $\mathscr{X}=C[0,1]$, from \S \ref{wiener}. Here,
$B:(\mathscr{X},\mathfrak{B}_{\mathscr{X}},W) \to (\mathscr{X},\mathfrak{B}_{\mathscr{X}})$
is $Bf=f$,  $P=B_* W = W$, and the covariance of $B$ is
$K:\mathscr{X}^* \to \mathscr{X}$,
\[
(K\mu)(t) = \int_{[0,1]} \min(s,t) d\mu(s),
\]
 satisfying
\[
\inner{K\mu}{\nu} = \mathbb{E}(\inner{B}{\mu},\inner{B}{\nu}).
\]


Take $\mathscr{H}=L^2[0,1]$, with Lebesgue measure. 
Define $L:\mathscr{H} \to \mathscr{X}$ by
\[
(L f)(t) = \int_0^t f(s) ds.
\]
Indeed, $Lf$ is continuous, and $L$ is linear and injective. 
$\mathscr{X}^*$ is the signed measures of bounded variation on $[0,1]$.
For $\mu \in \mathscr{X}^*$,  Fubini's theorem yields
\begin{align*}
 \inner{f}{L^*\mu}_{\mathscr{H}}& =
\inner{Lf}{\mu}\\
&=
\int_{[0,1]} (Lf)(t) d\mu(t)\\
&=\int_{[0,1]} \left( \int_0^t f(s) ds\right) d\mu(t)\\
&=\int_0^1 \left( \int_{[s,1]} d\mu(t) \right) f(s) ds\\
&=\int_0^1 \mu[s,1] f(s) ds\\
&=\inner{s \mapsto \mu[s,1]}{f}_{\mathscr{H}}.
\end{align*}
This shows that $L^*:\mathscr{X}^* \to \mathscr{H}$ is
\[
(L^*\mu)(s) = \mu[s,1].
\]

For $\mu \in \mathscr{X}^*$,
\begin{align*}
(LL^*\mu)(t) &= \int_0^t (L^*\mu)(s) ds\\
&=\int_0^t \mu[s,1] ds\\
&=\int_0^1 1_{[0,t]}(s) \left( \int_{[s,1]} d\mu(r) \right) ds\\
&=\int_{[0,1]}  \left( \int_0^r  1_{[0,t]}(s) ds \right) d\mu(r)\\
&=\int_{[0,1]} \min(r,t) d\mu(r),
\end{align*}
showing that $LL^*=K$. 
Then by Theorem \ref{factorization},
\[
H_W = L\mathscr{H}
\]
and
\[
\inner{F}{G}_{H_W} = \inner{L^{-1}F}{L^{-1}G}_{\mathscr{H}},
\]
for $F,G \in H_W$. This means that if $F \in H_W$ if and only if there is $f \in L^2[0,1]$ such that
\[
F(t) = (Lf)(t) = \int_0^t f(s) ds.
\]
This is equivalent to $F$ being absolutely continuous, with $F(0)=0$ and for almost all $t \in [0,1]$, $F$ is differentiable at $t$, and
$F' \in L^2[0,1]$.\footnote{\url{http://individual.utoronto.ca/jordanbell/notes/totalvariation.pdf}} 
Thus, $H_W$ is the collection of absolutely continuous functions $F:[0,1] \to \mathbb{R}$ satisfying $F(0)=0$ and $F' \in L^2[0,1]$. 
Furthermore,
\[
\inner{F}{G}_{H_W} = \inner{L^{-1}F}{L^{-1}G}_{L^2[0,1]} = \inner{F'}{G'}_{L^2[0,1]}.
\]



\section{The Cameron-Martin theorem}
Let $\mathscr{X}$ be a Fr\'echet space and let
$X:(\Omega,\mathfrak{F},\mathbb{P}) \to (\mathscr{X},\mathfrak{B}_{\mathscr{X}})$ be a random vector with distribution
$P=X_*\mathbb{P}$. 
For $h \in \mathscr{X}$, $X+h$ is a random vector, and we write $P_h=(X+h)_*\mathbb{P}$. For $A \in \mathfrak{B}_{\mathscr{X}}$,
\[
P_h(A) = \mathbb{P}(X+h \in A) = \mathbb{P}(X \in A-h) = P(A-h).
\]
If $P_h$ is absolutely continuous with respect to $P$, written
$P_h \ll P$, we say that $h$ is an \textbf{admissible shift}. 


For $\mathscr{X}=\mathbb{R}^d$, let $X$ be a random vector with state space
$\mathscr{X}$ and Gaussian distribution with $\mathbb{E} X=0$ and
covariance $I_d$, namely a random vector on $\mathscr{X}$ with the standard Gaussian distribution.
Let $\lambda_d$ be Lebesgue measure on $\mathbb{R}^d$. For $P=X_*\mathbb{P}$, which is a standard Gaussian
measure on $\mathbb{R}^d$, 
\[
dP(x) = (2\pi)^{-d/2} e^{-\inner{x}{x}/2} d\lambda_d(x).
\]
That is, the density of $P$ with respect to $\lambda_d$ is
\[
\frac{dP}{d\lambda_d}(x) =  (2\pi)^{-d/2} e^{-\inner{x}{x}/2}.
\]
For $h \in \mathbb{R}^d$ and $A$ a Borel set in $\mathbb{R}^d$,
\begin{align*}
P_h(A)& = P(A-h)\\
&= \int_{A-h}   (2\pi)^{-d/2} e^{-\inner{x}{x}/2} dx\\
&=\int_A (2\pi)^{-d/2} e^{-\inner{y-h}{y-h}/2} dy,
\end{align*}
which shows that
\[
\frac{dP_h}{d\lambda_d}(x) =  (2\pi)^{-d/2} e^{-\inner{x-h}{x-h}/2}.
\]
Because $\lambda_d \ll P$,
with
\[
\frac{d\lambda_d}{dP}(x) = (2\pi)^{d/2}  e^{\inner{x}{x}/2},
\]
the chain rule for the Radon-Nikodym derivative yields
\[
\frac{dP_h}{dP}(x) = \frac{dP_h}{d\lambda_d}(x) \cdot \frac{d\lambda_d}{dP}(x)
= (2\pi)^{-d/2} e^{-\inner{x-h}{x-h}/2} \cdot (2\pi)^{d/2}  e^{\inner{x}{x}/2},
\]
which is
\[
\frac{dP_h}{dP}(x)  = e^{\inner{h}{x}- \inner{h}{h}/2}.
\]





We now get to the \textbf{Cameron-Martin theorem}.\footnote{Mikhail Lifshits, {\em Lectures on Gaussian Processes},
p.~34, Theorem 5.1.}

\begin{theorem}[Cameron-Martin theorem]
Let $\mathscr{X}$ be a Fr\'echet space, let $X:(\Omega,\mathfrak{F},\mathbb{P}) \to (\mathscr{X},\mathfrak{B}_{\mathscr{X}})$
be a random vector that is Gaussian with $\mathbb{E} X = 0$ and covariance $K:\mathscr{X}^* \to \mathscr{X}$, and with
distribution $P=X_*\mathbb{P}$. 
In this case, $P_h \ll P$ if and only if $h \in H_P$.

If $h \in  H_P$, then there is some $\phi \in \mathscr{X}_P^*$ such that $L\phi=h$ and
\[
\frac{dP_h}{dP}(f) = e^{\inner{f}{\phi} - \frac{\inner{h}{h}_{H_P}}{2}},\qquad f \in \mathscr{X}.
\]
\end{theorem}


We have established that 
\[
H_W = \{h \in AC[0,1] : h(0) = 0, h' \in L^2[0,1]\}
\]
and 
\[
\norm{h}_{H_W}^2 = \int_0^1 |h'(s)|^2 ds,\qquad h \in H_W.
\]
For $h=H_W$ let $\phi \in L^2[0,1]$ such that $L\phi=h$,
i.e. $\phi=L^{-1}h$ which means $\phi=h'$ in $L^2[0,1]$. 






\end{document}